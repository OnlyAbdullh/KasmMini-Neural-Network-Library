# ğŸ§  KasmMiniNN

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Version](https://img.shields.io/badge/version-0.1.0-green)
![License](https://img.shields.io/badge/license-MIT-lightgrey)
![Status](https://img.shields.io/badge/status-educational-orange)

**KasmMiniNN** Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© Python Ø®ÙÙŠÙØ© Ù„Ø¨Ù†Ø§Ø¡ ÙˆØªØ¯Ø±ÙŠØ¨ Ø´Ø¨ÙƒØ§Øª Ø¹ØµØ¨ÙŠØ© Ø¨Ø³ÙŠØ·Ø© Ù…Ù† Ø§Ù„ØµÙØ±ØŒ  
Ù…ÙˆØ¬Ù‘Ù‡Ø© Ù„Ù„ØªØ¹Ù„Ù‘Ù…ØŒ Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ØŒ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø£Ø·Ø± Ø¬Ø§Ù‡Ø²Ø©.

> Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù‡Ùˆ ÙÙ‡Ù… Ù…Ø§ ÙŠØ­Ø¯Ø« Ø¯Ø§Ø®Ù„ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© (`forward` / `backward`) Ø¨Ø¯Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙƒØµÙ†Ø¯ÙˆÙ‚ Ø£Ø³ÙˆØ¯.

---

## ğŸ“Œ Ø§Ù„Ù…Ù„Ø®Ù‘Øµ

| Ø§Ù„Ø¹Ù†ØµØ±  | Ø§Ù„Ù‚ÙŠÙ…Ø©                               |
|--------|--------------------------------------|
| Ø§Ù„Ù†Ø³Ø®Ø© | `0.1.0`                              |
| Ø§Ù„Ù…Ø¤Ù„Ù | `OnlyOne`                            |
| Ø§Ù„ØºØ§ÙŠØ© | Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ§Øª Ø¹ØµØ¨ÙŠØ© ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ù„Ù„ØªØ¹Ù„Ù‘Ù… ÙˆØ§Ù„ØªØ¬Ø±Ø¨Ø© |

---

## âœ¨ Ø§Ù„Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©

- Ø·Ø¨Ù‚Ø© **`Dense`** Ù…Ø¹ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙˆØ§Ù„ØªØ¯Ø±Ø¬Ø§Øª  
- Ø¯ÙˆØ§Ù„ ØªÙ†Ø´ÙŠØ·:
  - `Relu()`, `LeakyReLU()`, `Sigmoid()`, `Tanh()`, `Linear()`
- Ø·Ø¨Ù‚Ø§Øª ØªÙ†Ø¸ÙŠÙ…:
  - `Dropout()`
  - `BatchNormalization()`
- Ø¯ÙˆØ§Ù„ Ø®Ø³Ø§Ø±Ø©:
  - `SoftmaxCrossEntropy`, `MeanSquaredError`, `BinaryCrossEntropy`
- Ù…Ø­Ø³Ù†Ø§Øª:
  - `SGD`, `Momentum`, `AdaGrad`, `Adam`
- ÙƒØ§Ø¦Ù† `NeuralNetwork` Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
- ÙƒØ§Ø¦Ù† `Trainer` Ù„Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ…
- `HyperparameterTuner` Ù„Ø¯Ø¹Ù…:
  - Grid Search
  - Random Search
  - K-Fold Cross Validation
- Ø£Ù…Ø«Ù„Ø© Ø¬Ø§Ù‡Ø²Ø© Ø¹Ù„Ù‰ **Iris** Ùˆ **MNIST**
- Ø£Ø¯ÙˆØ§Øª Ø±Ø³Ù… ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (`plotting.py`)

---

## ğŸ“¦ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª

- **Python:** 3.8+  
- Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª:
  - `numpy`
  - `scikit-learn`
  - `matplotlib` (Ù„Ù„Ø±Ø³ÙˆÙ… ÙÙ‚Ø·)

### Ø§Ù„ØªØ«Ø¨ÙŠØª
```bash
pip install numpy scikit-learn matplotlib

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­Ù„ÙŠ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹
git clone <repo-url>
cd <repo-directory>
pip install -e .

ğŸš€ Ø¯Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø³Ø±ÙŠØ¹
1ï¸âƒ£ ØªØ¯Ø±ÙŠØ¨ Ø¨Ø³ÙŠØ· (Iris)
from KasmMiniNN import (
    Dense, Sigmoid, Relu, BatchNormalization,
    SoftmaxCrossEntropy, NeuralNetwork, SGD, Trainer
)

# Ø§ÙØªØ±Ø¶ Ø£Ù† x_train, x_val, x_test, t_train, t_val, t_test Ø¬Ø§Ù‡Ø²ÙˆÙ† ÙˆÙ…Ù‡ÙŠØ£ÙˆÙ†
layers = [
    Dense(input_dim, 32),
    Sigmoid(),
    BatchNormalization(32),
    Dense(32, 16),
    Relu(),
    Dense(16, num_classes),
]

net = NeuralNetwork(layers, SoftmaxCrossEntropy())
optimizer = SGD(lr=0.1)

trainer = Trainer(
    network=net,
    optimizer=optimizer,
    x_train=x_train,
    t_train=t_train,
    x_val=x_val,
    t_val=t_val,
    x_test=x_test,
    t_test=t_test,
    epochs=20,
    batch_size=64,
)

history = trainer.fit()

2ï¸âƒ£ Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª (Hyperparameter Tuning)
from KasmMiniNN import HyperparameterTuner

def build_network_from_config(config):
    # Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø§Ø¹ØªÙ…Ø§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ config ÙˆØ¥Ø±Ø¬Ø§Ø¹ NeuralNetwork
    ...

tuner = HyperparameterTuner(
    build_network=lambda cfg: build_network_from_config(cfg),
    x_train=x_train, t_train=t_train,
    x_val=x_val, t_val=t_val,
)

results = tuner.grid_search(
    learning_rates=[1e-3, 1e-2],
    batch_sizes=[32, 64],
    hidden_sizes=[64, 128],
    optimizer_types=["adam"],
    dropout_rates=[0.0, 0.3],
    epochs_list=[10],
    num_layers_list=[1, 2],
    activation_types=["relu", "tanh"]
)

best_params = results["best_params"]

ğŸ“š Ù…Ø±Ø¬Ø¹ Ø³Ø±ÙŠØ¹ Ù„Ù„Ù€ API
NeuralNetwork
NeuralNetwork(layers, loss_layer)


ÙŠÙˆÙÙ‘Ø± Ø§Ù„ÙˆØ¸Ø§Ø¦Ù: forward, predict, loss, accuracy, gradient, init_weight.

Dense
Dense(input_size, output_size, weight_init="he", bias_init=0.)

Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙØ¹ÙŠÙ„

Relu()

LeakyReLU(alpha)

Sigmoid()

Tanh()

Linear()

Layers Ø§Ù„ØªÙ†Ø¸ÙŠÙ…

Dropout(dropout_ratio)

BatchNormalization(feature_size, momentum=0.9)

Ø¯ÙˆØ§Ù„ Ø§Ù„Ø®Ø³Ø§Ø±Ø©
Ø§Ù„Ø¯Ø§Ù„Ø©	Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
SoftmaxCrossEntropy	ØªØµÙ†ÙŠÙ Ù…ØªØ¹Ø¯Ø¯
MeanSquaredError	Ø§Ù†Ø­Ø¯Ø§Ø±
BinaryCrossEntropy	ØªØµÙ†ÙŠÙ Ø«Ù†Ø§Ø¦ÙŠ
Optimizers

SGD(lr)

Momentum(lr, momentum=0.9)

AdaGrad(lr)

Adam(lr, beta1=0.9, beta2=0.999)

ğŸ§© Ø§Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ (Ø¨Ø¥ÙŠØ¬Ø§Ø²)

ÙƒÙ„ Layer ÙŠÙ…Ù„Ùƒ forward Ùˆ backward.

NeuralNetwork.gradient: forward â†’ loss â†’ backward.

Trainer ÙŠØ¯ÙŠØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªÙ‚ÙŠÙŠÙ….

BatchNormalization ÙŠØ¯Ø¹Ù… ÙˆØ¶Ø¹ÙŠØªÙŠ training Ùˆ evaluation.

Dropout ÙŠÙÙØ¹Ù„ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙ‚Ø·.

ğŸ› ï¸ Ø­Ù„ÙˆÙ„ Ù„Ù…Ø´Ø§ÙƒÙ„ Ø´Ø§Ø¦Ø¹Ø©

Ø®Ø·Ø£:

RuntimeError: forward must be called before backward


Ø§Ù„Ø­Ù„: ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ forward Ø£Ùˆ loss Ù‚Ø¨Ù„ backward.

Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯:

ØªØ­Ù‚Ù‚ Ù…Ù† x.shape[0] == t.shape[0].

ØªØ£ÙƒØ¯ Ù…Ù† ØªÙ†Ø³ÙŠÙ‚ t (labels Ø£Ùˆ one-hot).

â–¶ï¸ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø«Ù„Ø©
python example.py


Ø³ÙŠÙØ·Ù„Ø¨ Ù…Ù†Ùƒ:

Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: Iris Ø£Ùˆ MNIST

ÙˆØ¶Ø¹ Ø§Ù„ØªØ´ØºÙŠÙ„: train, tune, random, kfold

Ø±Ø³Ù… ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¯Ø±ÙŠØ¨
from KasmMiniNN.plotting import plot_history
plot_history(history)

ğŸ¤ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø©

Ù…Ø±Ø­Ø¨ Ø¨Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø§Øª:

ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡

ØªÙˆØ«ÙŠÙ‚ Ø£ÙØ¶Ù„

Ø¥Ø¶Ø§ÙØ© Layers Ø£Ùˆ Optimizers Ø¬Ø¯ÙŠØ¯Ø©

ğŸ“Œ ÙŠÙØ¶Ù‘Ù„ Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ÙˆØ­Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… pytest.

ğŸ“„ Ø§Ù„ØªØ±Ø®ÙŠØµ

MIT License â€” Ø¶Ø¹ Ù…Ù„Ù LICENSE ÙÙŠ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹.

ğŸ“¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„

Ø§Ù„Ù…Ø¤Ù„Ù: OnlyOne

Ø§Ù„Ø¥ØµØ¯Ø§Ø±: 0.1.0

Ø§Ø¨Ø¯Ø£ Ø§Ù„Ø¢Ù† â€” Ø§ÙÙ‡Ù… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù…Ù† Ø§Ù„Ø¯Ø§Ø®Ù„ Ø¨Ø¯Ù„ Ø§Ù„Ø§ÙƒØªÙØ§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§.